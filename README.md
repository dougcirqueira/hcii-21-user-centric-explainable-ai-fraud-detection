# hcii-21-user-centric-explainable-ai-fraud-detection
Systematic literature review data, implementation and resources for the development of the paper "Towards Design Principles for User-Centric Explainable AI in Fraud Detection" submitted to HCI International 2021 by:
- Douglas Cirqueira, School of Computing, Dublin City University and Lero - the Science Foundation Ireland Research Centre for Software, Dublin City University, Dublin, Ireland
- Markus Helfert, Innovation Value Institute, Maynooth University and Lero - the Science Foundation Ireland Research Centre for Software, Dublin City University, Dublin, Ireland
- Marija Bezbradica, School of Computing, Dublin City University and Lero - the Science Foundation Ireland Research Centre for Software, Dublin City University, Dublin, Ireland

# Simulation Results 
The Python script and dataset adopted to perform the user simulated experiment, and instantiate explanation methods is available at the [resources folder.](https://github.com/dougcirqueira/hcii-21-user-centric-explainable-ai-fraud-detection/tree/main/resources/simulation) 

# Results of Systematic Literature Review: Identifying Design Features of Explanation Methods
The complete set of papers and statistics for the literature review are available at the [resources folder.](https://github.com/dougcirqueira/hcii-21-user-centric-explainable-ai-fraud-detection/tree/main/resources/systematic_literature_review)

# Table with Events Analyzed for the Systematic Literature Review
![Image of Events](https://github.com/dougcirqueira/hcii-21-user-centric-explainable-ai-fraud-detection/blob/main/resources/systematic_literature_review/events_HCII_21.png)

# Table with Selected Papers and Design Features

|paper_data     |paper_data                                                                                                               |paper_data|paper_data                                                                                                                                                                                            |design_features_explanation_methods       |DF          |DF              |DF                  |DF                           |DF    |DF                  |DF              |
|---------------|---------------------------------------------------------------------------------------------------------------------|------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------|----------------|--------------------|------------------------|---------------------------------|-----------|-------------------------|---------------------|
|database_source|paper_source                                                                                                         |year  |title                                                                                                                                                                                             |df1_prediction_probabilities_AI_efficiency|df2_repres_cases|df3_dissimilar_cases|df4_cases_mistaking_pred|df5_data_feats_affect_predictions|df6_what_if|df7_rules_conditions_pred|df8_feats_impact_pred|
|scopus         |                                                                                                                     |2019  |explainability in human–agent systems                                                                                                                                                             |                                          |x               |                    |                        |                                 |           |x                        |x                    |
|scopus         |                                                                                                                     |2019  |can we do better explanations? a proposal of usercentered explainable ai                                                                                                                          |                                          |                |                    |x                       |                                 |           |x                        |x                    |
|scopus         |                                                                                                                     |2019  |how to achieve explainability and transparency in human ai interaction                                                                                                                            |                                          |                |                    |                        |                                 |           |x                        |                     |
|scopus         |                                                                                                                     |2019  |exploring the need for explainable artificial intelligence (xai) in intelligent tutoring systems (its)                                                                                            |                                          |                |                    |                        |                                 |           |x                        |                     |
|scopus         |                                                                                                                     |2019  |counterfactuals in explainable artificial intelligence (xai) evidence from human reasoning                                                                                                        |                                          |                |                    |                        |                                 |x          |                         |                     |
|scopus         |                                                                                                                     |2018  |improving user trust on deep neural networks based intrusion detection systems                                                                                                                    |                                          |                |                    |                        |x                                |           |x                        |                     |
|scopus         |                                                                                                                     |2018  |believe it or not designing a humanai partnership for mixedinitiative factchecking                                                                                                                |x                                         |                |                    |                        |                                 |           |                         |x                    |
|scopus         |                                                                                                                     |2018  |using perceptual and cognitive explanations for enhanced humanagent team performance                                                                                                              |                                          |                |                    |                        |                                 |x          |x                        |                     |
|scopus         |                                                                                                                     |2018  |improving the explainability of random forest classifier – user centered approach                                                                                                                 |                                          |                |                    |                        |x                                |           |                         |x                    |
|scopus         |                                                                                                                     |2019  |constraintsbased explanations of classifications                                                                                                                                                  |                                          |                |                    |x                       |x                                |           |x                        |                     |
|scopus         |                                                                                                                     |2019  |explanations of blackbox model predictions by contextual importance and utility                                                                                                                   |x                                         |x               |                    |                        |x                                |           |x                        |x                    |
|scopus         |                                                                                                                     |2019  |physiological indicators for user trust in machine learning with influence enhanced factchecking                                                                                                  |                                          |x               |                    |                        |                                 |           |                         |x                    |
|scopus         |                                                                                                                     |2018  |human knowledge in constructing ai systems neural logic networks approach towards an explainable ai                                                                                               |                                          |                |                    |                        |                                 |           |x                        |x                    |
|scopus         |                                                                                                                     |2018  |Human-in-the-loop interpretability prior                                                                                                                                                          |                                          |                |                    |                        |                                 |           |x                        |                     |
|scopus         |                                                                                                                     |2018  |Towards robust interpretability with self-explaining neuralnetworks                                                                                                                               |                                          |                |                    |                        |x                                |           |                         |x                    |
|scopus         |                                                                                                                     |2016  |Generating visual explanations                                                                                                                                                                    |                                          |x               |                    |x                       |                                 |           |x                        |                     |
|scopus         |                                                                                                                     |2018  |Methods for interpreting and understanding deepneural networks                                                                                                                                    |                                          |x               |                    |                        |x                                |           |                         |                     |
|scopus         |                                                                                                                     |2019  |The effects of example-based explanations in a machine learninginterface                                                                                                                          |                                          |x               |                    |                        |                                 |           |                         |                     |
|scopus         |                                                                                                                     |2020  |A study on trust in black box models and post-hoc explanations                                                                                                                                    |                                          |                |                    |                        |x                                |           |x                        |                     |
|scopus         |                                                                                                                     |2018  |Teaching categories to human learners with visual explanations                                                                                                                                    |                                          |x               |                    |                        |x                                |           |                         |x                    |
|scopus         |                                                                                                                     |2019  |Machine Learning Interpretability: A Survey on Methods and Metrics                                                                                                                                |                                          |x               |                    |                        |x                                |x          |x                        |x                    |
|scopus         |                                                                                                                     |2018  |Asking ’why’ in ai: Explain-ability of intelligent systems–perspectives and challenges                                                                                                            |x                                         |x               |                    |                        |x                                |           |x                        |x                    |
|acm            |                                                                                                                     |2019  |on human predictions with explanations and predictions of machine learning models a case study on deception detection                                                                             |x                                         |x               |x                   |                        |x                                |           |                         |x                    |
|acm            |                                                                                                                     |2017  |A bayesianframework for learning rule sets for interpretable classification                                                                                                                       |                                          |                |                    |                        |                                 |           |x                        |                     |
|acm            |                                                                                                                     |2018  |A Survey of Methods for Explaining Black Box Models                                                                                                                                               |                                          |x               |x                   |                        |x                                |           |x                        |x                    |
|acm            |                                                                                                                     |2019  |What can AI do for me: Evaluating Machine Learning Interpretations in Cooperative Play                                                                                                            |x                                         |x               |                    |                        |x                                |           |                         |                     |
|acm            |                                                                                                                     |2019  | Explaining Decision-Making Algorithmsthrough UI: Strategies to Help Non-Expert Stakeholders                                                                                                      |                                          |                |                    |                        |x                                |x          |                         |                     |
|acm            |                                                                                                                     |2016  |Interpretable Decision Sets: A Joint Framework for Description and Prediction                                                                                                                     |                                          |                |                    |                        |x                                |           |x                        |                     |
|scopus         |                                                                                                                     |2019  |Factual and counterfactual explanations for black box decision making                                                                                                                             |                                          |                |                    |                        |                                 |x          |x                        |                     |
|scopus         |                                                                                                                     |2019  |RetainVis: Visual Analytics with Interpretable and Interactive RecurrentNeural Networks on Electronic Medical Records                                                                             |x                                         |                |                    |                        |x                                |x          |                         |x                    |
|scopus         |                                                                                                                     |2018  |Learning Fuzzy Relations and Properties for Explainable Artificial Intelligence                                                                                                                   |                                          |x               |                    |                        |                                 |           |x                        |                     |
|springer       |                                                                                                                     |2019  |detecting potential local adversarial examples for humaninterpretable defense                                                                                                                     |                                          |                |                    |x                       |                                 |           |                         |x                    |
|springer       |                                                                                                                     |2018  |humanmachine teaming and cyberspace                                                                                                                                                               |                                          |x               |                    |                        |x                                |           |x                        |                     |
|springer       |                                                                                                                     |2020  |One explanation does not fit all                                                                                                                                                                  |                                          |x               |                    |                        |x                                |x          |x                        |x                    |
|arXiv          |                                                                                                                     |2019  |Interpreting Machine Learning Malware Detectors Which Leverage N-gram Analysis                                                                                                                    |                                          |                |                    |                        |x                                |           |x                        |                     |
|arXiv          |                                                                                                                     |2020  |Explainable Active Learning (XAL): An Empirical Study of How Local Explanations Impact Annotator Experience                                                                                       |                                          |x               |                    |                        |x                                |           |                         |x                    |
|arXiv          |                                                                                                                     |2020  |Explaining Data-Driven Decisions made by AI Systems: The Counterfactual Approach                                                                                                                  |                                          |x               |                    |                        |x                                |x          |x                        |x                    |
|arXiv          |                                                                                                                     |2018  |Why X rather than Y? Explaining Neural Model' Predictions by Generating Intervention Counterfactual Samples                                                                                       |                                          |x               |x                   |x                       |                                 |x          |x                        |                     |
|arXiv          |                                                                                                                     |2019  |Explainable AI for Intelligence Augmentation in Multi-Domain Operations                                                                                                                           |                                          |                |                    |                        |x                                |           |                         |x                    |
|scopus         |                                                                                                                     |2019  |explAIner: A Visual Analytics Framework for Interactive and Explainable Machine Learning                                                                                                          |x                                         |                |                    |                        |x                                |           |x                        |x                    |
|arXiv          |                                                                                                                     |2019  |How model accuracy and explanation fidelity influence user trust                                                                                                                                  |x                                         |x               |x                   |                        |x                                |x          |x                        |                     |
|scopus         |                                                                                                                     |2019  |Explaining Deep Classification of Time-Series Data with Learned Prototypes                                                                                                                        |                                          |x               |                    |                        |                                 |           |                         |                     |
|arXiv          |                                                                                                                     |2019  |Natural Language Interaction with Explainable AI Models                                                                                                                                           |                                          |                |                    |                        |x                                |x          |x                        |                     |
|arXiv          |                                                                                                                     |2019  |Explanation in Human-AI Systems: A Literature Meta-Review, Synopsis of Key Ideas and Publications, and Bibliography for Explainable AI                                                            |                                          |x               |x                   |                        |x                                |x          |x                        |                     |
|arXiv          |                                                                                                                     |2018  |An Adversarial Approach for Explainable AI in Intrusion Detection Systems                                                                                                                         |                                          |                |                    |x                       |x                                |           |                         |x                    |
|arXiv          |                                                                                                                     |2018  |Explainable artificial intelligence (XAI), the goodness criteria and the grasp-ability test                                                                                                       |                                          |                |                    |                        |x                                |x          |x                        |x                    |
|scopus         |                                                                                                                     |2019  |Techniques for Interpretable Machine Learning                                                                                                                                                     |                                          |                |                    |x                       |x                                |x          |x                        |                     |
|springer       |                                                                                                                     |2018  |Explanation Methods in Deep Learning: Users, Values, Concerns and Challenges                                                                                                                      |                                          |                |                    |x                       |x                                |           |x                        |x                    |
|arXiv          |                                                                                                                     |2017  |What do we need to build explainable AI systems for the medical domain?                                                                                                                           |                                          |                |                    |                        |x                                |           |x                        |x                    |
|arXiv          |                                                                                                                     |2017  |What Does Explainable AI Really Mean? A New Conceptualization of Perspectives                                                                                                                     |                                          |                |                    |                        |x                                |           |x                        |                     |
|arXiv          |                                                                                                                     |2019  |Towards Quantification of Explainability in Explainable Artificial Intelligence Methods                                                                                                           |x                                         |                |                    |                        |x                                |           |x                        |x                    |
|arXiv          |                                                                                                                     |2019  |Domain Knowledge Aided Explainable Artificial Intelligence for Intrusion Detection and Response                                                                                                   |                                          |                |                    |                        |x                                |           |                         |x                    |
|arXiv          |                                                                                                                     |2020  |Questioning the AI: Informing Design Practices for Explainable AI User Experiences                                                                                                                |                                          |x               |                    |                        |x                                |x          |x                        |x                    |
|arXiv          |                                                                                                                     |2020  |Evaluating saliency map explanations for convolutional neural networks: a user study                                                                                                              |                                          |                |                    |                        |x                                |           |x                        |x                    |
|arXiv          |                                                                                                                     |2020  |Interpretable Machine Learning Model for Early Prediction of Mortality in Elderly Patients with Multiple Organ Dysfunction Syndrome (MODS): a Multicenter Retrospective Study and Cross Validation|                                          |                |                    |                        |x                                |           |                         |x                    |
|arXiv          |                                                                                                                     |2020  |Iterative and Adaptive Sampling with Spatial Attention for Black-Box Model Explanations                                                                                                           |                                          |x               |                    |                        |x                                |           |                         |x                    |
|arXiv          |                                                                                                                     |2020  |Feature relevance quantification in explainable AI: A causal problem                                                                                                                              |                                          |-               |                    |                        |x                                |           |                         |                     |
|arXiv          |                                                                                                                     |2019  |Asymmetric Shapley values: incorporating causal knowledge into model-agnostic explainability                                                                                                      |                                          |-               |                    |                        |x                                |           |                         |x                    |
|scopus         |                                                                                                                     |2019  |Towards Explainable Artificial Intelligence                                                                                                                                                       |                                          |x               |                    |                        |x                                |           |                         |                     |
|arXiv          |                                                                                                                     |2019  |Explaining Deep Neural Networks Using Spectrum-Based Fault Localization                                                                                                                           |                                          |-               |                    |x                       |x                                |           |                         |                     |
|arXiv          |                                                                                                                     |2019  |A Survey on Explainable Artificial Intelligence (XAI): Towards Medical XAI                                                                                                                        |                                          |x               |                    |                        |x                                |x          |x                        |x                    |
|arXiv          |                                                                                                                     |2019  |Generating User-friendly Explanations for Loan Denials using GANs                                                                                                                                 |                                          |x               |                    |x                       |x                                |x          |x                        |                     |
|arXiv          |                                                                                                                     |2019  |Visualizing the decision-making process in deep neural decision forest                                                                                                                            |                                          |x               |                    |                        |x                                |           |                         |x                    |
|arXiv          |                                                                                                                     |2018  |Explainable Text Classification in Legal Document Review A Case Study of Explainable Predictive Coding                                                                                            |                                          |-               |                    |                        |x                                |           |x                        |                     |
|scopus         |                                                                                                                     |2019  |Solving the Black Box Problem: A Normative Framework for Explainable Artificial Intelligence                                                                                                      |                                          |-               |                    |x                       |x                                |           |x                        |                     |
|arXiv          |                                                                                                                     |2019  |Interpretable machine learning: definitions, methods, and applications                                                                                                                            |x                                         |-               |                    |                        |x                                |           |x                        |                     |
|arXiv          |                                                                                                                     |2018  |RISE: Randomized Input Sampling for Explanation of Black-box Models                                                                                                                               |x                                         |x               |                    |                        |x                                |           |x                        |                     |
|arXiv          |                                                                                                                     |2016  |Interpretable Machine Learning Models for the Digital Clock Drawing Test                                                                                                                          |                                          |-               |                    |                        |x                                |           |x                        |                     |
|backward       |explainability in human–agent systems                                                                                |2010  |How to explain individual classification decisions                                                                                                                                                |                                          |x               |                    |                        |x                                |           |                         |                     |
|backward       |explainability in human–agent systems                                                                                |2011  |Prototype selection for interpretable classification                                                                                                                                              |                                          |x               |                    |                        |                                 |           |                         |                     |
|backward       |explainability in human–agent systems                                                                                |1999  |Extracting decision trees from trained neural networks                                                                                                                                            |                                          |x               |                    |                        |                                 |           |x                        |                     |
|backward       |explainability in human–agent systems                                                                                |2013  |Using sensitivity analysis and visualization techniques to open black boxdata mining models                                                                                                       |                                          |-               |                    |                        |x                                |           |x                        |x                    |
|backward       |explainability in human–agent systems                                                                                |2019  |Interpreting tree ensembles with intrees                                                                                                                                                          |                                          |-               |                    |                        |                                 |           |x                        |                     |
|backward       |explainability in human–agent systems                                                                                |2017  |Interpretable explanations of black boxes by meaningful perturbation                                                                                                                              |                                          |x               |                    |x                       |x                                |           |                         |x                    |
|backward       |explainability in human–agent systems                                                                                |2016  |Examples are not enough, learn to criticize! criticism forinterpretability                                                                                                                        |                                          |x               |x                   |                        |x                                |           |                         |                     |
|backward       |explainability in human–agent systems                                                                                |2015  |The bayesian case model: A generative approach for case-basedreasoning and prototype classification                                                                                               |                                          |x               |                    |                        |x                                |           |x                        |                     |
|backward       |explainability in human–agent systems                                                                                |2016  |Interacting with predictions: Visual inspection of black-box machinelearning models                                                                                                               |x                                         |-               |                    |                        |x                                |x          |                         |x                    |
|backward       |explainability in human–agent systems                                                                                |2015  |Interpretable classifiers usingrules and bayesian analysis:  Building a better stroke prediction model                                                                                            |                                          |-               |                    |                        |                                 |           |x                        |                     |
|backward       |explainability in human–agent systems                                                                                |2012  |Intelligible models for classification and regression                                                                                                                                             |                                          |-               |                    |                        |x                                |           |                         |                     |
|backward       |explainability in human–agent systems                                                                                |2019  |Accurate intelligible models with pairwiseinteraction                                                                                                                                             |                                          |-               |                    |                        |                                 |           |                         |x                    |
|backward       |explainability in human–agent systems                                                                                |2017  |A unified approach to interpreting model predictions                                                                                                                                              |                                          |-               |                    |                        |x                                |           |                         |x                    |
|backward       |explainability in human–agent systems                                                                                |2017  |Grad-cam:  Visual explanations from deep networks via gradient-based localizatio                                                                                                                  |                                          |-               |                    |x                       |x                                |           |                         |x                    |
|backward       |explainability in human–agent systems                                                                                |2020  |Tree space prototypes: Another look at making tree ensemblesinterpretable                                                                                                                         |x                                         |x               |                    |                        |x                                |           |x                        |                     |
|backward       |explainability in human–agent systems                                                                                |2017  |Interpretable predictions of tree-basedensembles via actionable feature tweaking                                                                                                                  |                                          |-               |                    |                        |x                                |           |                         |                     |
|backward       |explainability in human–agent systems                                                                                |2018  |Explicating feature contribution using random forestproximity distances                                                                                                                           |x                                         |x               |                    |                        |x                                |           |                         |                     |
|backward       |“hello ai” uncovering the onboarding needs of medical practitioners for human–ai collaborative decisionmaking        |2017  |Understanding black-box predictions via influence functions                                                                                                                                       |x                                         |-               |                    |x                       |                                 |x          |                         |                     |
|backward       |can we do better explanations? a proposal of usercentered explainable ai                                             |2017  |Counterfactual Explanations Without Opening the Black Box: AutomatedDecisions and the GDPR                                                                                                        |                                          |-               |                    |x                       |                                 |x          |                         |                     |
|backward       |how to achieve explainability and transparency in human ai interaction                                               |2014  |Explaining prediction models and individual predictions with feature contributions                                                                                                                |x                                         |-               |                    |                        |x                                |           |                         |x                    |
|backward       |using perceptual and cognitive explanations for enhanced humanagent team performance                                 |2018  |How do humans understand explanations from machine learning systems? An evaluation of the human-interpretability of explanation                                                                   |                                          |-               |                    |                        |                                 |x          |x                        |                     |
|backward       |improving the explainability of random forest classifier – user centered approach                                    |2015  |Extracting Rule RF in Educational Data Classification: From a Random Forest to Interpretable Refined Rules                                                                                        |                                          |-               |                    |                        |                                 |           |x                        |                     |
|backward       |detecting potential local adversarial examples for humaninterpretable defense                                        |2016  |Making tree ensembles interpretable                                                                                                                                                               |                                          |-               |                    |                        |                                 |           |x                        |                     |
|backward       |on human predictions with explanations and predictions of machine learning models a case study on deception detection|2018  |Anchors: High-Precision Model-Agnostic Explanations                                                                                                                                               |                                          |-               |                    |                        |x                                |           |x                        |x                    |
|backward       |Explainable Active Learning (XAL): An Empirical Study of How Local Explanations Impact Annotator Experience          |2018  |Manifold: A model-agnostic framework for interpretation and diagnosis of machinelearning models                                                                                                   |                                          |-               |                    |                        |x                                |           |                         |x                    |
|backward       |Explaining Data-Driven Decisions made by AI Systems: The Counterfactual Approach                                     |2019  |Counterfactualexplanation algorithms for behavioral and textual data                                                                                                                              |                                          |-               |                    |                        |                                 |x          |x                        |x                    |
|backward       |Explaining Data-Driven Decisions made by AI Systems: The Counterfactual Approach                                     |2009  |Explaining instance classificationswith interactions of subsets of feature values                                                                                                                 |x                                         |-               |                    |                        |x                                |           |                         |x                    |
|backward       |Explaining Data-Driven Decisions made by AI Systems: The Counterfactual Approach                                     |2019  |Interpretable counterfactual explanations guided by prototypes                                                                                                                                    |                                          |x               |                    |                        |                                 |x          |                         |                     |
|backward       |Why X rather than Y? Explaining Neural Model' Predictions by Generating Intervention Counterfactual Samples          |2019  | TabNet: Attentive Interpretable TabularLearning                                                                                                                                                  |                                          |-               |                    |                        |x                                |           |                         |                     |
|backward       |Why X rather than Y? Explaining Neural Model' Predictions by Generating Intervention Counterfactual Samples          |2018  | Exact andconsistent interpretation for piecewise linear neural networks: A closed formsolution                                                                                                   |                                          |x               |                    |                        |x                                |           |                         |                     |
|backward       |explAIner: A Visual Analytics Framework for Interactive and Explainable Machine Learning                             |2016  |Not Just ABlack Box: Learning Important Features Through Propagating ActivationDifferences                                                                                                        |                                          |-               |                    |                        |x                                |           |                         |                     |
|events         |How model accuracy and explanation fidelity influence user trust                                                     |2018  |Generating counterfactual explanations with natural language                                                                                                                                      |x                                         |x               |                    |                        |                                 |x          |x                        |x                    |
|backward       |How model accuracy and explanation fidelity influence user trust                                                     |2018  |A survey of interpretability and explainability in human-agent systems                                                                                                                            |                                          |x               |                    |                        |x                                |           |x                        |                     |
|backward       |Explaining Deep Classification of Time-Series Data with Learned Prototypes                                           |2018  |Deep learning for case-based reasoningthrough prototypes: A neural network that explains its pre-dictions                                                                                         |                                          |x               |                    |                        |                                 |           |                         |                     |
|backward       |Explanation Methods in Deep Learning: Users, Values, Concerns and Challenges                                         |2017  |Towards interpretable deep neural networksby leveraging adversarial examples                                                                                                                      |                                          |-               |                    |x                       |                                 |           |                         |                     |
|backward       |Explanation Methods in Deep Learning: Users, Values, Concerns and Challenges                                         |2017  |Improving interpretability of deep neuralnetworks  with  semantic  information                                                                                                                    |                                          |x               |                    |                        |x                                |           |x                        |                     |
|backward       |Explanation Methods in Deep Learning: Users, Values, Concerns and Challenges                                         |2017  |Learning important features through propagating activation differences                                                                                                                            |                                          |-               |                    |                        |x                                |           |                         |                     |
|backward       |What do we need to build explainable AI systems for the medical domain?                                              |2015  |On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation                                                                                                |                                          |-               |                    |                        |x                                |           |                         |x                    |
|backward       |What do we need to build explainable AI systems for the medical domain?                                              |2014  |Visualizing and understanding convolutional networks                                                                                                                                              |                                          |                |                    |                        |x                                |           |                         |x                    |
|forward        |“hello ai” uncovering the onboarding needs of medical practitioners for human–ai collaborative decisionmaking        |2020  |Interpreting Interpretability: Understanding Data Scientists' Use of Interpretability Tools for Machine Learning                                                                                  |x                                         |-               |                    |                        |x                                |           |                         |x                    |
|forward        |exploring the need for explainable artificial intelligence (xai) in intelligent tutoring systems (its)               |2020  |Explainable Artificial Intelligence: a Systematic Review                                                                                                                                          |x                                         |x               |x                   |x                       |x                                |x          |x                        |x                    |
|forward        |counterfactuals in explainable artificial intelligence (xai) evidence from human reasoning                           |2020  |Explainable reinforcement learning through a causal lens                                                                                                                                          |                                          |-               |                    |                        |                                 |x          |x                        |x                    |
|forward        |counterfactuals in explainable artificial intelligence (xai) evidence from human reasoning                           |2019  |Exploratory Not Explanatory: Counterfactual Analysis of Saliency Maps for Deep RL                                                                                                                 |                                          |-               |                    |                        |x                                |x          |                         |                     |
|forward        |counterfactuals in explainable artificial intelligence (xai) evidence from human reasoning                           |2020  |Good Counterfactuals and Where to Find Them: A Case-Based Technique for Generating Counterfactuals for Explainable AI (XAI)                                                                       |                                          |-               |                    |                        |                                 |x          |                         |                     |
|forward        |believe it or not designing a humanai partnership for mixedinitiative factchecking                                   |2019  |Can you explain that? Lucid explanations help human-AI collaborative image retrieval                                                                                                              |                                          |-               |                    |                        |x                                |           |                         |                     |
|forward        |2d transparency space—bring domain users and machine learning experts together                                       |2013  |Too much, too little, or just right? Ways explanations impact end users’ mental models                                                                                                            |x                                         |x               |x                   |                        |x                                |           |                         |                     |
|forward        |on human predictions with explanations and predictions of machine learning models a case study on deception detection|2019  |Learning to deceive with attention-based explanations                                                                                                                                             |                                          |-               |                    |                        |x                                |           |                         |                     |
|forward        |on human predictions with explanations and predictions of machine learning models a case study on deception detection|2019  |Many Faces of Feature Importance: Comparing Built-in and Post-hoc Feature Importance in Text Classification                                                                                       |                                          |x               |                    |                        |x                                |           |x                        |                     |
|forward        |on human predictions with explanations and predictions of machine learning models a case study on deception detection|2020  |Harnessing Explanations to Bridge AI and Humans                                                                                                                                                   |                                          |x               |                    |                        |x                                |           |                         |                     |
|forward        |on human predictions with explanations and predictions of machine learning models a case study on deception detection|2020  |Feature-Based Explanations Don't Help People Detect Misclassifications of Online Toxicity                                                                                                         |x                                         |-               |                    |                        |x                                |           |                         |                     |
|forward        |on human predictions with explanations and predictions of machine learning models a case study on deception detection|2019  |The Design and Evaluation of Neural Attention Mechanisms for Explaining Text Classifiers                                                                                                          |                                          |x               |                    |x                       |x                                |           |                         |x                    |
|forward        |Explaining Deep Classification of Time-Series Data with Learned Prototypes                                           |2019  |Towards a rigorous evaluation of XAI Methods on Time Series                                                                                                                                       |                                          |-               |                    |                        |x                                |           |                         |                     |
|events         |                                                                                                                     |2017  |Interpretable & Explorable Approximations of Black Box Models                                                                                                                                     |                                          |-               |                    |                        |                                 |           |x                        |                     |
|events         |                                                                                                                     |2019  |Efficient Search for Diverse Coherent Explanations                                                                                                                                                |                                          |-               |                    |                        |x                                |x          |x                        |                     |
|events         |                                                                                                                     |2016  |Model-Agnostic Interpretability of Machine Learning                                                                                                                                               |x                                         |-               |                    |                        |x                                |           |x                        |x                    |
|events         |                                                                                                                     |2019  |Transparency: Motivations and Challenges                                                                                                                                                          |                                          |-               |                    |                        |x                                |x          |                         |x                    |
|events         |                                                                                                                     |2018  |Instance-Level Explanations for Fraud Detection: A Case Study                                                                                                                                     |x                                         |-               |                    |                        |x                                |           |x                        |x                    |
|arXiv          |                                                                                                                     |2018  |Contrastive Explanations with Local Foil Trees                                                                                                                                                    |                                          |x               |                    |                        |x                                |           |x                        |x                    |
|events         |                                                                                                                     |2018  |Defining Locality for Surrogates in Post-hoc Interpretablity                                                                                                                                      |                                          |-               |                    |                        |x                                |           |                         |x                    |
|events         |                                                                                                                     |2016  |An unexpected unity among methods for interpreting model predictions                                                                                                                              |                                          |-               |                    |                        |x                                |           |                         |x                    |
|events         |                                                                                                                     |2016  |Nothing Else Matters: Model-Agnostic Explanations By Identifying Prediction Invariance                                                                                                            |x                                         |-               |                    |                        |x                                |           |x                        |x                    |
|events         |                                                                                                                     |2016  |Programs as Black-Box Explanations                                                                                                                                                                |                                          |                |                    |                        |                                 |           |x                        |                     |
|events         |                                                                                                                     |2017  |Explanation and Justification in Machine Learning: A Survey                                                                                                                                       |                                          |-               |                    |                        |x                                |           |x                        |                     |
|events         |                                                                                                                     |2017  |Explainable AI: Beware of Inmates Running the Asylum. Or: How I Learnt to Stop Worrying and Love the Social and Behavioural Sciences                                                              |                                          |x               |                    |                        |                                 |x          |                         |x                    |
|events         |                                                                                                                     |2019  |Towards a Generic Framework for Explanations of Black-box Algorithmic Decision Systems                                                                                                            |                                          |x               |x                   |                        |x                                |x          |x                        |x                    |
|events         |                                                                                                                     |2017  |Requirements for Conceptual Representations of Explanations and How Reasoning Systems CanServe Them                                                                                               |                                          |-               |                    |                        |                                 |           |x                        |x                    |
|scopus         |                                                                                                                     |2018  |Explainable AI: The New 42?                                                                                                                                                                       |                                          |-               |                    |                        |x                                |           |x                        |                     |
|scopus         |                                                                                                                     |2018  |Evaluating Explanations by Cognitive Value                                                                                                                                                        |x                                         |-               |                    |                        |x                                |x          |x                        |                     |
|scopus         |                                                                                                                     |2018  |Measures of Model Interpretability for Model Selection                                                                                                                                            |                                          |-               |                    |                        |                                 |           |x                        |x                    |
|events         |                                                                                                                     |2019  |Using Relational Concept Networks for Explainable Decision Support                                                                                                                                |                                          |-               |                    |                        |x                                |           |x                        |x                    |
|events         |                                                                                                                     |2019  |How to Improve the Adaptation Phase of the CBR in the Medical Domain                                                                                                                              |                                          |x               |x                   |                        |                                 |           |x                        |                     |
|scopus         |                                                                                                                     |2019  |A Case for Guided Machine Learning                                                                                                                                                                |                                          |-               |                    |                        |                                 |           |x                        |                     |
|scopus         |                                                                                                                     |2018  |Visualizations for an Explainable Planning Agent                                                                                                                                                  |                                          |-               |                    |                        |x                                |           |                         |                     |
|events         |                                                                                                                     |2019  |Human-Understandable Explanations of Infeasibility for Resource-Constrained Scheduling Problems                                                                                                   |                                          |-               |                    |                        |                                 |           |x                        |                     |
|events         |                                                                                                                     |2019  |Towards Model-Based Contrastive Explanations for Explainable Planning                                                                                                                             |                                          |-               |                    |                        |                                 |x          |                         |                     |
|events         |                                                                                                                     |2018  |Explaining Recommendations Using Contexts                                                                                                                                                         |                                          |x               |                    |                        |                                 |           |x                        |x                    |
|scopus         |                                                                                                                     |2019  |I can do better than your AI: expertise and explanations                                                                                                                                          |x                                         |-               |                    |                        |x                                |           |                         |                     |
|scopus         |                                                                                                                     |2019  |The effects of example-based explanations in a machine learning interface                                                                                                                         |                                          |x               |                    |                        |                                 |           |                         |                     |
|scopus         |                                                                                                                     |2019  |Automated rationale generation: a technique for explainable AI and its effects on human perceptions                                                                                               |                                          |x               |                    |                        |                                 |           |x                        |                     |
|events         |                                                                                                                     |2020  |AutoAIViz: opening the blackbox of automated artificial intelligence with conditional parallel coordinates                                                                                        |x                                         |-               |                    |                        |x                                |           |                         |                     |
|events         |                                                                                                                     |2020  |Leveraging rationales to improve human task performance                                                                                                                                           |                                          |x               |x                   |                        |                                 |           |x                        |                     |
|events         |                                                                                                                     |2020  |ViCE: visual counterfactual explanations for machine learning models                                                                                                                              |x                                         |-               |                    |                        |x                                |x          |                         |                     |
|scopus         |                                                                                                                     |2018  |The design and validation of an intuitive confidence measure                                                                                                                                      |                                          |x               |x                   |                        |                                 |           |                         |                     |
|scopus         |                                                                                                                     |2019  |Towards an Explainable Threat Detection Tool                                                                                                                                                      |x                                         |x               |x                   |x                       |x                                |           |                         |                     |
|scopus         |                                                                                                                     |2019  |Why these Explanations? Selecting Intelligibility Types for Explanation Goals                                                                                                                     |x                                         |x               |                    |                        |x                                |x          |x                        |                     |
|springer       |                                                                                                                     |2018  |Tell Me Why: Computational Explanation of Conceptual Similarity Judgments                                                                                                                         |x                                         |x               |                    |                        |x                                |           |x                        |                     |
|scopus         |                                                                                                                     |2018  |Comparison-Based Inverse Classification for Interpretability in Machine Learning                                                                                                                  |                                          |x               |                    |                        |x                                |           |                         |x                    |
|events         |                                                                                                                     |2018  |Explainability through Transparency and User Control: A Case-Based Recommender for Engineering Worke                                                                                              |                                          |x               |                    |                        |                                 |           |                         |                     |
|events         |                                                                                                                     |2018  |Measuring explanation quality in XCBR                                                                                                                                                             |                                          |x               |                    |                        |                                 |           |                         |                     |
|events         |                                                                                                                     |2019  |Recent Trends in XAI: A Broad Overview on Current Approaches, Methodologies and Interactions                                                                                                      |                                          |-               |x                   |                        |                                 |           |x                        |x                    |
|scopus         |                                                                                                                     |2020  |sentiment analysis using rulebased and casebased reasoning                                                                                                                                        |                                          |x               |                    |                        |                                 |           |x                        |                     |
|scopus         |                                                                                                                     |2018  |explainable sentiment analysis with applications in medicine                                                                                                                                      |                                          |                |                    |                        |x                                |           |x                        |                     |
|scopus         |                                                                                                                     |2019  |using aspectbased analysis for explainable sentiment predictions                                                                                                                                  |                                          |                |                    |                        |x                                |           |                         |                     |
|scopus         |                                                                                                                     |2019  |explaining sentiment classification                                                                                                                                                               |x                                         |                |                    |                        |x                                |           |                         |x                    |
|scopus         |                                                                                                                     |2020  |the whatif tool interactive probing of machine learning models                                                                                                                                    |                                          |x               |                    |                        |                                 |x          |                         |                     |
|scopus         |                                                                                                                     |2019  |personalizing the prediction interactive and interpretable machine learning                                                                                                                       |                                          |                |                    |                        |x                                |           |                         |                     |
|scopus         |                                                                                                                     |2019  |interpretable machine learning with boosting by boolean algorithm                                                                                                                                 |                                          |                |                    |                        |                                 |           |                         |x                    |
|scopus         |                                                                                                                     |2019  |application of interpretable machine learning models for the intelligent decision                                                                                                                 |                                          |                |                    |                        |x                                |           |                         |                     |
|scopus         |                                                                                                                     |2018  |interpretable machine learning techniques for causal inference using balancing scores as metafeatures                                                                                             |                                          |                |                    |                        |                                 |           |                         |x                    |
|scopus         |                                                                                                                     |2018  |why the failure? how adversarial examples can provide insights for interpretable machine learning                                                                                                 |                                          |                |                    |x                       |                                 |           |                         |                     |
|scopus         |                                                                                                                     |2018  |interpretable machine learning convolutional neural networks with rbf fuzzy logic classification rules                                                                                            |                                          |                |                    |                        |                                 |           |x                        |                     |
|scopus         |                                                                                                                     |2018  |easyminereu web framework for interpretable machine learning based on rules and frequent itemsets                                                                                                 |x                                         |                |                    |                        |                                 |           |x                        |                     |
|arxiv          |                                                                                                                     |2019  |A Grounded Interaction Protocol for Explainable Artificial Intelligence                                                                                                                           |                                          |                |                    |                        |                                 |x          |                         |                     |
|arxiv          |                                                                                                                     |2019  |Efficient Saliency Maps for Explainable AI                                                                                                                                                        |                                          |                |                    |                        |x                                |           |                         |                     |
|arxiv          |                                                                                                                     |2019  |Rule Extraction in Unsupervised Anomaly Detection for Model Explainability: Application to OneClass SVM                                                                                           |                                          |x               |                    |                        |                                 |x          |x                        |                     |
|arxiv          |                                                                                                                     |2019  |Digital Twin approach to Clinical DSS with Explainable AI                                                                                                                                         |x                                         |                |                    |                        |x                                |           |x                        |                     |
|arxiv          |                                                                                                                     |2019  |X-ToM: Explaining with Theory-of-Mind for Gaining Justified Human Trust                                                                                                                           |                                          |                |                    |                        |x                                |           |x                        |x                    |
|arxiv          |                                                                                                                     |2019  |A Human-Grounded Evaluation of SHAP for Alert Processing                                                                                                                                          |x                                         |                |                    |                        |                                 |           |                         |x                    |
|arxiv          |                                                                                                                     |2019  |Generating Counterfactual and Contrastive Explanations using SHAP                                                                                                                                 |                                          |                |                    |                        |                                 |x          |x                        |                     |
|arxiv          |                                                                                                                     |2019  |Local Score Dependent Model Explanation for Time Dependent Covariates                                                                                                                             |                                          |                |                    |                        |                                 |           |                         |x                    |
|arxiv          |                                                                                                                     |2019  |Do Human Rationales Improve Machine Explanations?                                                                                                                                                 |                                          |                |                    |                        |x                                |           |                         |                     |
|arxiv          |                                                                                                                     |2019  |Explainable AI for Trees: From Local Explanations to Global Understanding                                                                                                                         |x                                         |                |                    |                        |x                                |           |                         |x                    |
|arxiv          |                                                                                                                     |2018  |Contrastive explanations for reinforcement learning in terms of expected consequences                                                                                                             |                                          |x               |x                   |                        |                                 |           |x                        |                     |
|arxiv          |                                                                                                                     |2018  |Grounding Visual Explanations                                                                                                                                                                     |x                                         |x               |x                   |                        |                                 |x          |x                        |                     |
|arxiv          |                                                                                                                     |2020  |Using Explainable Artificial Intelligence to Increase Trust in Computer Vision                                                                                                                    |                                          |                |                    |                        |                                 |           |                         |x                    |
|arxiv          |                                                                                                                     |2020  |Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI                                                                               |x                                         |x               |                    |x                       |x                                |x          |x                        |x                    |
|arxiv          |                                                                                                                     |2019  |Propagated Perturbation of Adversarial Attack for well-known CNNs: Empirical Study and its Explanation                                                                                            |                                          |                |                    |x                       |                                 |           |                         |                     |
|scopus         |                                                                                                                     |2019  |Induction of Non-monotonic Logic Programs To Explain Statistical Learning Models                                                                                                                  |x                                         |                |                    |                        |                                 |           |                         |x                    |
|arxiv          |                                                                                                                     |2018  |A Multidisciplinary Survey and Framework for Design and Evaluation of Explainable AI Systems                                                                                                      |                                          |x               |                    |x                       |x                                |x          |x                        |                     |
|arxiv          |                                                                                                                     |2018  |Explanations for temporal recommendations                                                                                                                                                         |x                                         |x               |                    |                        |                                 |           |                         |                     |
|arxiv          |                                                                                                                     |2018  |Learning from the machine: interpreting machine learning algorithms for point- and extended- source classification                                                                                |                                          |                |                    |                        |                                 |           |x                        |x                    |
|scopus         |                                                                                                                     |2020  |Interpretable machine learning for privacy-preserving pervasive systems                                                                                                                           |x                                         |x               |                    |                        |                                 |x          |                         |x                    |
|scopus         |                                                                                                                     |2017  |" What is relevant in a text document?": An interpretable machine learning approach                                                                                                               |                                          |                |                    |                        |                                 |           |                         |x                    |
